{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7f78b0-d7fd-4922-84a1-a2c45851bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/28 17:41:06 WARN DependencyUtils: Local jar /opt/spark/postgresql-42.7.8.jar does not exist, skipping.\n",
      "25/12/28 17:41:06 INFO SparkContext: Running Spark version 3.4.0\n",
      "25/12/28 17:41:06 INFO ResourceUtils: ==============================================================\n",
      "25/12/28 17:41:06 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/12/28 17:41:06 INFO ResourceUtils: ==============================================================\n",
      "25/12/28 17:41:06 INFO SparkContext: Submitted application: pyspark-shell\n",
      "25/12/28 17:41:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/12/28 17:41:06 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/12/28 17:41:06 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/12/28 17:41:06 INFO SecurityManager: Changing view acls to: root\n",
      "25/12/28 17:41:06 INFO SecurityManager: Changing modify acls to: root\n",
      "25/12/28 17:41:06 INFO SecurityManager: Changing view acls groups to: \n",
      "25/12/28 17:41:06 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/12/28 17:41:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/12/28 17:41:07 INFO Utils: Successfully started service 'sparkDriver' on port 34123.\n",
      "25/12/28 17:41:07 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/12/28 17:41:07 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/12/28 17:41:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/12/28 17:41:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/12/28 17:41:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/12/28 17:41:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3a64611c-258a-48ce-8cf4-59c8660ef576\n",
      "25/12/28 17:41:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/12/28 17:41:07 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/12/28 17:41:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/12/28 17:41:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/28 17:41:07 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/12/28 17:41:07 ERROR SparkContext: Failed to add /opt/spark/postgresql-42.7.8.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/spark/postgresql-42.7.8.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1968)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2023)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/12/28 17:41:07 INFO Executor: Starting executor ID driver on host f30b651f709e\n",
      "25/12/28 17:41:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/12/28 17:41:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39427.\n",
      "25/12/28 17:41:07 INFO NettyBlockTransferService: Server created on f30b651f709e:39427\n",
      "25/12/28 17:41:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/12/28 17:41:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f30b651f709e, 39427, None)\n",
      "25/12/28 17:41:07 INFO BlockManagerMasterEndpoint: Registering block manager f30b651f709e:39427 with 434.4 MiB RAM, BlockManagerId(driver, f30b651f709e, 39427, None)\n",
      "25/12/28 17:41:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f30b651f709e, 39427, None)\n",
      "25/12/28 17:41:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f30b651f709e, 39427, None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/postgresql-42.7.8.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac864d3-33a1-4cad-a843-d2066936d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/12/28 17:41:08 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.\n",
      "25/12/28 17:41:10 INFO InMemoryFileIndex: It took 98 ms to list leaf files for 1 paths.\n",
      "25/12/28 17:41:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:41:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:41:14 INFO CodeGenerator: Code generated in 286.993136 ms\n",
      "25/12/28 17:41:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.8 KiB, free 434.2 MiB)\n",
      "25/12/28 17:41:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)\n",
      "25/12/28 17:41:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:41:14 INFO SparkContext: Created broadcast 0 from showString at <unknown>:0\n",
      "25/12/28 17:41:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:41:14 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:41:14 INFO DAGScheduler: Got job 0 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:41:14 INFO DAGScheduler: Final stage: ResultStage 0 (showString at <unknown>:0)\n",
      "25/12/28 17:41:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:41:14 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.9 KiB, free 434.2 MiB)\n",
      "25/12/28 17:41:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f30b651f709e:39427 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:41:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:41:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:41:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:41:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/12/28 17:41:15 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:41:15 INFO CodeGenerator: Code generated in 37.123603 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+------------+------+\n",
      "|       u|       g|       r|       i|       z|    redshift| class|\n",
      "+--------+--------+--------+--------+--------+------------+------+\n",
      "|19.34492|16.92385|15.77771|15.33381|15.12316|-5.516215E-6|  STAR|\n",
      "|24.65397|22.53888|20.80799|19.86182|19.34411|   0.4979803|GALAXY|\n",
      "|21.42899|20.91531|20.86941|20.80266|20.38503|    2.442472|   QSO|\n",
      "|22.83521|21.55918|21.22711|20.94139|20.87671|-9.738771E-4|  STAR|\n",
      "| 23.0417|22.13414| 20.4698|18.59738|17.57369|-6.033703E-5|  STAR|\n",
      "+--------+--------+--------+--------+--------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2044 bytes result sent to driver\n",
      "25/12/28 17:41:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 381 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:41:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:15 INFO DAGScheduler: ResultStage 0 (showString at <unknown>:0) finished in 0.600 s\n",
      "25/12/28 17:41:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:41:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/12/28 17:41:15 INFO DAGScheduler: Job 0 finished: showString at <unknown>:0, took 0.669556 s\n",
      "25/12/28 17:41:15 INFO CodeGenerator: Code generated in 26.311567 ms\n"
     ]
    }
   ],
   "source": [
    "schema_ddl = \"u DOUBLE, g DOUBLE, r DOUBLE, i DOUBLE, z DOUBLE, redshift DOUBLE, class STRING\"\n",
    "df = spark.read.csv(\"raw_data2.csv\", header=True, schema=schema_ddl)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a3cf87-882f-4fad-9936-202bae9c1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+------+------------+\n",
      "|       u|       g|       r|       i|       z| class|    redshift|\n",
      "+--------+--------+--------+--------+--------+------+------------+\n",
      "|19.34492|16.92385|15.77771|15.33381|15.12316|  STAR|-5.516215E-6|\n",
      "|24.65397|22.53888|20.80799|19.86182|19.34411|GALAXY|   0.4979803|\n",
      "|21.42899|20.91531|20.86941|20.80266|20.38503|   QSO|    2.442472|\n",
      "|22.83521|21.55918|21.22711|20.94139|20.87671|  STAR|-9.738771E-4|\n",
      "| 23.0417|22.13414| 20.4698|18.59738|17.57369|  STAR|-6.033703E-5|\n",
      "+--------+--------+--------+--------+--------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:41:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:41:16 INFO CodeGenerator: Code generated in 22.677244 ms\n",
      "25/12/28 17:41:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)\n",
      "25/12/28 17:41:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:16 INFO SparkContext: Created broadcast 2 from showString at <unknown>:0\n",
      "25/12/28 17:41:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:41:16 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Got job 1 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Final stage: ResultStage 1 (showString at <unknown>:0)\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.9 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on f30b651f709e:39427 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:41:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:41:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:41:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/12/28 17:41:16 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:41:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1997 bytes result sent to driver\n",
      "25/12/28 17:41:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 59 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:41:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:16 INFO DAGScheduler: ResultStage 1 (showString at <unknown>:0) finished in 0.076 s\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:41:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/12/28 17:41:16 INFO DAGScheduler: Job 1 finished: showString at <unknown>:0, took 0.084047 s\n",
      "25/12/28 17:41:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on f30b651f709e:39427 in memory (size: 7.8 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"u\", \"g\", \"r\", \"i\", \"z\", \"class\", \"redshift\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c44a6c-fdc5-40ce-bf90-f751787a8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- u: double (nullable = true)\n",
      " |-- g: double (nullable = true)\n",
      " |-- r: double (nullable = true)\n",
      " |-- i: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- redshift: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6aafe6-0145-4525-b1bf-ac49ae8b6d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:18 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:41:18 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(7, u#0, g#1, r#2, i#3, z#4, class#6, redshift#5)\n",
      "25/12/28 17:41:18 INFO CodeGenerator: Code generated in 20.756622 ms\n",
      "25/12/28 17:41:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.8 KiB, free 433.7 MiB)\n",
      "25/12/28 17:41:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.7 MiB)\n",
      "25/12/28 17:41:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:18 INFO SparkContext: Created broadcast 4 from count at <unknown>:0\n",
      "25/12/28 17:41:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:41:18 INFO DAGScheduler: Registering RDD 11 (count at <unknown>:0) as input to shuffle 0\n",
      "25/12/28 17:41:18 INFO DAGScheduler: Got map stage job 2 (count at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:41:18 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at <unknown>:0)\n",
      "25/12/28 17:41:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:41:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:18 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 19.6 KiB, free 433.7 MiB)\n",
      "25/12/28 17:41:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 433.7 MiB)\n",
      "25/12/28 17:41:18 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on f30b651f709e:39427 (size: 9.2 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:18 INFO BlockManagerInfo: Removed broadcast_2_piece0 on f30b651f709e:39427 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:18 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:41:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on f30b651f709e:39427 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 6) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 8) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 9) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "25/12/28 17:41:18 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
      "25/12/28 17:41:18 INFO Executor: Running task 2.0 in stage 2.0 (TID 4)\n",
      "25/12/28 17:41:18 INFO Executor: Running task 3.0 in stage 2.0 (TID 5)\n",
      "25/12/28 17:41:18 INFO Executor: Running task 4.0 in stage 2.0 (TID 6)\n",
      "25/12/28 17:41:18 INFO Executor: Running task 5.0 in stage 2.0 (TID 7)\n",
      "25/12/28 17:41:18 INFO Executor: Running task 6.0 in stage 2.0 (TID 8)\n",
      "25/12/28 17:41:18 INFO Executor: Running task 7.0 in stage 2.0 (TID 9)\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 17346848-21683560, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 13010136-17346848, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 4336712-8673424, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 21683560-26020272, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 8673424-13010136, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 30356984-30499392, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 26020272-30356984, partition values: [empty row]\n",
      "25/12/28 17:41:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f30b651f709e:39427 in memory (size: 34.3 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:41:18 INFO Executor: Finished task 7.0 in stage 2.0 (TID 9). 2070 bytes result sent to driver\n",
      "25/12/28 17:41:18 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 9) in 471 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:41:19 INFO Executor: Finished task 3.0 in stage 2.0 (TID 5). 2070 bytes result sent to driver\n",
      "25/12/28 17:41:19 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 1511 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:41:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2027 bytes result sent to driver\n",
      "25/12/28 17:41:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1525 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:41:19 INFO Executor: Finished task 2.0 in stage 2.0 (TID 4). 2027 bytes result sent to driver\n",
      "25/12/28 17:41:19 INFO Executor: Finished task 4.0 in stage 2.0 (TID 6). 2027 bytes result sent to driver\n",
      "25/12/28 17:41:19 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 1541 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:41:19 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 6) in 1540 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:41:19 INFO Executor: Finished task 6.0 in stage 2.0 (TID 8). 2027 bytes result sent to driver\n",
      "25/12/28 17:41:19 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 2027 bytes result sent to driver\n",
      "25/12/28 17:41:19 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 8) in 1565 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:41:19 INFO Executor: Finished task 5.0 in stage 2.0 (TID 7). 2027 bytes result sent to driver\n",
      "25/12/28 17:41:19 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 1576 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:41:19 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 7) in 1572 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:41:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:19 INFO DAGScheduler: ShuffleMapStage 2 (count at <unknown>:0) finished in 1.633 s\n",
      "25/12/28 17:41:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:41:19 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:41:19 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:41:19 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:41:19 INFO CodeGenerator: Code generated in 19.657449 ms\n",
      "25/12/28 17:41:19 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "25/12/28 17:41:19 INFO DAGScheduler: Got job 3 (count at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:41:19 INFO DAGScheduler: Final stage: ResultStage 4 (count at <unknown>:0)\n",
      "25/12/28 17:41:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "25/12/28 17:41:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:20 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.1 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on f30b651f709e:39427 (size: 5.8 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:41:20 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:41:20 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:41:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 10) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "25/12/28 17:41:20 INFO Executor: Running task 0.0 in stage 4.0 (TID 10)\n",
      "25/12/28 17:41:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on f30b651f709e:39427 in memory (size: 9.2 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:41:20 INFO ShuffleBlockFetcherIterator: Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 25 ms\n",
      "25/12/28 17:41:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 10). 4038 bytes result sent to driver\n",
      "25/12/28 17:41:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 10) in 150 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:41:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:20 INFO DAGScheduler: ResultStage 4 (count at <unknown>:0) finished in 0.183 s\n",
      "25/12/28 17:41:20 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:41:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "25/12/28 17:41:20 INFO DAGScheduler: Job 3 finished: count at <unknown>:0, took 0.209194 s\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=df.columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29fd08a-4d9b-4b15-96ef-a6a40c5b42c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:41:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:41:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/12/28 17:41:21 INFO CodeGenerator: Code generated in 46.701593 ms\n",
      "25/12/28 17:41:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)\n",
      "25/12/28 17:41:21 INFO BlockManagerInfo: Removed broadcast_6_piece0 on f30b651f709e:39427 in memory (size: 5.8 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:41:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:21 INFO SparkContext: Created broadcast 7 from count at <unknown>:0\n",
      "25/12/28 17:41:21 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:41:21 INFO DAGScheduler: Registering RDD 18 (count at <unknown>:0) as input to shuffle 1\n",
      "25/12/28 17:41:21 INFO DAGScheduler: Got map stage job 4 (count at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:41:21 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (count at <unknown>:0)\n",
      "25/12/28 17:41:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:41:21 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:21 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[18] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:21 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 38.4 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:21 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:21 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on f30b651f709e:39427 (size: 17.1 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:21 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:21 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[18] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:41:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 12) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 13) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 14) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 15) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 16) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 17) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 18) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:21 INFO Executor: Running task 0.0 in stage 5.0 (TID 11)\n",
      "25/12/28 17:41:21 INFO Executor: Running task 3.0 in stage 5.0 (TID 14)\n",
      "25/12/28 17:41:21 INFO Executor: Running task 7.0 in stage 5.0 (TID 18)\n",
      "25/12/28 17:41:21 INFO Executor: Running task 6.0 in stage 5.0 (TID 17)\n",
      "25/12/28 17:41:21 INFO Executor: Running task 5.0 in stage 5.0 (TID 16)\n",
      "25/12/28 17:41:21 INFO Executor: Running task 1.0 in stage 5.0 (TID 12)\n",
      "25/12/28 17:41:21 INFO Executor: Running task 2.0 in stage 5.0 (TID 13)\n",
      "25/12/28 17:41:21 INFO Executor: Running task 4.0 in stage 5.0 (TID 15)\n",
      "25/12/28 17:41:21 INFO CodeGenerator: Code generated in 37.676981 ms\n",
      "25/12/28 17:41:21 INFO CodeGenerator: Code generated in 8.022726 ms\n",
      "25/12/28 17:41:21 INFO CodeGenerator: Code generated in 17.353177 ms\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 4336712-8673424, partition values: [empty row]\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 17346848-21683560, partition values: [empty row]\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 30356984-30499392, partition values: [empty row]\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 8673424-13010136, partition values: [empty row]\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 21683560-26020272, partition values: [empty row]\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 13010136-17346848, partition values: [empty row]\n",
      "25/12/28 17:41:21 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 26020272-30356984, partition values: [empty row]\n",
      "25/12/28 17:41:22 INFO Executor: Finished task 7.0 in stage 5.0 (TID 18). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:22 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 18) in 749 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:41:22 INFO BlockManagerInfo: Removed broadcast_4_piece0 on f30b651f709e:39427 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 6.0 in stage 5.0 (TID 17). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 17) in 2172 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 1.0 in stage 5.0 (TID 12). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 3.0 in stage 5.0 (TID 14). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 2.0 in stage 5.0 (TID 13). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 12) in 2188 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 13) in 2189 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 14) in 2189 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 4.0 in stage 5.0 (TID 15). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 15) in 2193 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 0.0 in stage 5.0 (TID 11). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 2202 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 5.0 in stage 5.0 (TID 16). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 16) in 2204 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:41:23 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:23 INFO DAGScheduler: ShuffleMapStage 5 (count at <unknown>:0) finished in 2.233 s\n",
      "25/12/28 17:41:23 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:41:23 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:41:23 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:41:23 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:41:23 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 3285665, minimum partition size: 1048576\n",
      "25/12/28 17:41:23 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/12/28 17:41:23 INFO CodeGenerator: Code generated in 30.499928 ms\n",
      "25/12/28 17:41:23 INFO DAGScheduler: Registering RDD 21 (count at <unknown>:0) as input to shuffle 2\n",
      "25/12/28 17:41:23 INFO DAGScheduler: Got map stage job 5 (count at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:41:23 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (count at <unknown>:0)\n",
      "25/12/28 17:41:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "25/12/28 17:41:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:23 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[21] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:23 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 45.9 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:23 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.9 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on f30b651f709e:39427 (size: 20.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:23 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:23 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[21] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:41:23 INFO TaskSchedulerImpl: Adding task set 7.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 19) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 20) (f30b651f709e, executor driver, partition 1, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 21) (f30b651f709e, executor driver, partition 2, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 22) (f30b651f709e, executor driver, partition 3, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 23) (f30b651f709e, executor driver, partition 4, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 24) (f30b651f709e, executor driver, partition 5, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 25) (f30b651f709e, executor driver, partition 6, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 26) (f30b651f709e, executor driver, partition 7, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:41:23 INFO Executor: Running task 0.0 in stage 7.0 (TID 19)\n",
      "25/12/28 17:41:23 INFO Executor: Running task 1.0 in stage 7.0 (TID 20)\n",
      "25/12/28 17:41:23 INFO Executor: Running task 2.0 in stage 7.0 (TID 21)\n",
      "25/12/28 17:41:23 INFO Executor: Running task 4.0 in stage 7.0 (TID 23)\n",
      "25/12/28 17:41:23 INFO Executor: Running task 3.0 in stage 7.0 (TID 22)\n",
      "25/12/28 17:41:23 INFO Executor: Running task 5.0 in stage 7.0 (TID 24)\n",
      "25/12/28 17:41:23 INFO Executor: Running task 6.0 in stage 7.0 (TID 25)\n",
      "25/12/28 17:41:23 INFO Executor: Running task 7.0 in stage 7.0 (TID 26)\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.5 MiB) non-empty blocks including 8 (3.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:23 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
      "25/12/28 17:41:23 INFO Executor: Finished task 0.0 in stage 7.0 (TID 19). 5681 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 19) in 348 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 2.0 in stage 7.0 (TID 21). 5681 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 3.0 in stage 7.0 (TID 22). 5681 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 22) in 359 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 21) in 380 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 7.0 in stage 7.0 (TID 26). 5638 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 26) in 400 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 6.0 in stage 7.0 (TID 25). 5638 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 5.0 in stage 7.0 (TID 24). 5638 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 25) in 413 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 4.0 in stage 7.0 (TID 23). 5638 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 23) in 420 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 24) in 418 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 1.0 in stage 7.0 (TID 20). 5638 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 20) in 431 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:41:24 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:24 INFO DAGScheduler: ShuffleMapStage 7 (count at <unknown>:0) finished in 0.447 s\n",
      "25/12/28 17:41:24 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:41:24 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:41:24 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:41:24 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:41:24 INFO BlockManagerInfo: Removed broadcast_8_piece0 on f30b651f709e:39427 in memory (size: 17.1 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:24 INFO CodeGenerator: Code generated in 10.621317 ms\n",
      "25/12/28 17:41:24 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Got job 6 (count at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Final stage: ResultStage 10 (count at <unknown>:0)\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[24] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:24 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.1 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:24 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:24 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on f30b651f709e:39427 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:24 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[24] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:41:24 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 27) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "25/12/28 17:41:24 INFO Executor: Running task 0.0 in stage 10.0 (TID 27)\n",
      "25/12/28 17:41:24 INFO ShuffleBlockFetcherIterator: Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/12/28 17:41:24 INFO Executor: Finished task 0.0 in stage 10.0 (TID 27). 3995 bytes result sent to driver\n",
      "25/12/28 17:41:24 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 27) in 18 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:41:24 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:24 INFO DAGScheduler: ResultStage 10 (count at <unknown>:0) finished in 0.030 s\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:41:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "25/12/28 17:41:24 INFO DAGScheduler: Job 6 finished: count at <unknown>:0, took 0.037242 s\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0053e2d1-af42-4e2f-948d-428a6d72a18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[u: double, g: double, r: double, i: double, z: double, redshift: double, class: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['u', 'g', 'r', 'i', 'z', 'redshift', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d764c1ed-fff2-4b95-ac28-c413171a3e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:26 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:41:26 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:41:26 INFO CodeGenerator: Code generated in 47.165933 ms\n",
      "25/12/28 17:41:26 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 199.8 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:26 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:26 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:26 INFO SparkContext: Created broadcast 11 from showString at <unknown>:0\n",
      "25/12/28 17:41:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:41:26 INFO DAGScheduler: Registering RDD 28 (showString at <unknown>:0) as input to shuffle 3\n",
      "25/12/28 17:41:26 INFO DAGScheduler: Got map stage job 7 (showString at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:41:26 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (showString at <unknown>:0)\n",
      "25/12/28 17:41:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:41:26 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:26 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[28] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:26 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 37.1 KiB, free 433.8 MiB)\n",
      "25/12/28 17:41:26 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 433.8 MiB)\n",
      "25/12/28 17:41:26 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on f30b651f709e:39427 (size: 17.7 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:26 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:26 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[28] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:41:26 INFO TaskSchedulerImpl: Adding task set 11.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 28) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 29) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 30) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 31) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 32) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 33) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 34) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 35) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:41:26 INFO Executor: Running task 1.0 in stage 11.0 (TID 29)\n",
      "25/12/28 17:41:26 INFO Executor: Running task 2.0 in stage 11.0 (TID 30)\n",
      "25/12/28 17:41:26 INFO Executor: Running task 5.0 in stage 11.0 (TID 33)\n",
      "25/12/28 17:41:26 INFO Executor: Running task 6.0 in stage 11.0 (TID 34)\n",
      "25/12/28 17:41:26 INFO Executor: Running task 7.0 in stage 11.0 (TID 35)\n",
      "25/12/28 17:41:26 INFO Executor: Running task 4.0 in stage 11.0 (TID 32)\n",
      "25/12/28 17:41:26 INFO Executor: Running task 3.0 in stage 11.0 (TID 31)\n",
      "25/12/28 17:41:26 INFO Executor: Running task 0.0 in stage 11.0 (TID 28)\n",
      "25/12/28 17:41:26 INFO CodeGenerator: Code generated in 12.876438 ms\n",
      "25/12/28 17:41:26 INFO CodeGenerator: Code generated in 6.176866 ms\n",
      "25/12/28 17:41:26 INFO CodeGenerator: Code generated in 8.253484 ms\n",
      "25/12/28 17:41:27 INFO CodeGenerator: Code generated in 9.149644 ms\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 8673424-13010136, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 4336712-8673424, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 17346848-21683560, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 21683560-26020272, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 26020272-30356984, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 13010136-17346848, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 30356984-30499392, partition values: [empty row]\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 7.0 in stage 11.0 (TID 35). 2722 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 35) in 170 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:41:27 INFO BlockManagerInfo: Removed broadcast_10_piece0 on f30b651f709e:39427 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:27 INFO BlockManagerInfo: Removed broadcast_9_piece0 on f30b651f709e:39427 in memory (size: 20.9 KiB, free: 434.3 MiB)\n",
      "[Stage 11:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| class| count|\n",
      "+------+------+\n",
      "|GALAXY|302320|\n",
      "|   QSO| 93706|\n",
      "|  STAR|103974|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:27 INFO Executor: Finished task 1.0 in stage 11.0 (TID 29). 2808 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 29) in 825 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 4.0 in stage 11.0 (TID 32). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 0.0 in stage 11.0 (TID 28). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 32) in 833 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 5.0 in stage 11.0 (TID 33). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 28) in 840 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 33) in 837 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 3.0 in stage 11.0 (TID 31). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 2.0 in stage 11.0 (TID 30). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 31) in 851 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 30) in 854 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 6.0 in stage 11.0 (TID 34). 2765 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 34) in 861 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:41:27 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:27 INFO DAGScheduler: ShuffleMapStage 11 (showString at <unknown>:0) finished in 0.878 s\n",
      "25/12/28 17:41:27 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:41:27 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:41:27 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:41:27 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:41:27 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/12/28 17:41:27 INFO BlockManagerInfo: Removed broadcast_7_piece0 on f30b651f709e:39427 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:27 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/12/28 17:41:27 INFO CodeGenerator: Code generated in 18.608737 ms\n",
      "25/12/28 17:41:27 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Got job 8 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Final stage: ResultStage 13 (showString at <unknown>:0)\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:27 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.9 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:27 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 434.1 MiB)\n",
      "25/12/28 17:41:27 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on f30b651f709e:39427 (size: 18.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:27 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:41:27 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 36) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "25/12/28 17:41:27 INFO Executor: Running task 0.0 in stage 13.0 (TID 36)\n",
      "25/12/28 17:41:27 INFO ShuffleBlockFetcherIterator: Getting 8 (1728.0 B) non-empty blocks including 8 (1728.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:41:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/12/28 17:41:27 INFO Executor: Finished task 0.0 in stage 13.0 (TID 36). 5154 bytes result sent to driver\n",
      "25/12/28 17:41:27 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 36) in 26 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:41:27 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:27 INFO DAGScheduler: ResultStage 13 (showString at <unknown>:0) finished in 0.037 s\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:41:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "25/12/28 17:41:27 INFO DAGScheduler: Job 8 finished: showString at <unknown>:0, took 0.044497 s\n",
      "25/12/28 17:41:27 INFO CodeGenerator: Code generated in 7.035599 ms             \n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30ae81c-8d14-4119-8afe-9b828e4477ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+------+------------+\n",
      "|       u|       g|       r|       i|       z| class|    redshift|\n",
      "+--------+--------+--------+--------+--------+------+------------+\n",
      "|19.34492|16.92385|15.77771|15.33381|15.12316|  STAR|-5.516215E-6|\n",
      "|24.65397|22.53888|20.80799|19.86182|19.34411|GALAXY|   0.4979803|\n",
      "|21.42899|20.91531|20.86941|20.80266|20.38503|   QSO|    2.442472|\n",
      "|22.83521|21.55918|21.22711|20.94139|20.87671|  STAR|-9.738771E-4|\n",
      "| 23.0417|22.13414| 20.4698|18.59738|17.57369|  STAR|-6.033703E-5|\n",
      "+--------+--------+--------+--------+--------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:41:28 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:41:28 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:41:28 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 199.8 KiB, free 433.9 MiB)\n",
      "25/12/28 17:41:28 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.8 MiB)\n",
      "25/12/28 17:41:28 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:28 INFO SparkContext: Created broadcast 14 from showString at <unknown>:0\n",
      "25/12/28 17:41:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:41:28 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Got job 9 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Final stage: ResultStage 14 (showString at <unknown>:0)\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[35] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:41:28 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.9 KiB, free 433.8 MiB)\n",
      "25/12/28 17:41:28 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.8 MiB)\n",
      "25/12/28 17:41:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on f30b651f709e:39427 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:41:28 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[35] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:41:28 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:41:28 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 37) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:41:28 INFO Executor: Running task 0.0 in stage 14.0 (TID 37)\n",
      "25/12/28 17:41:28 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:41:28 INFO Executor: Finished task 0.0 in stage 14.0 (TID 37). 1954 bytes result sent to driver\n",
      "25/12/28 17:41:28 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 37) in 30 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:41:28 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:41:28 INFO DAGScheduler: ResultStage 14 (showString at <unknown>:0) finished in 0.038 s\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:41:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "25/12/28 17:41:28 INFO DAGScheduler: Job 9 finished: showString at <unknown>:0, took 0.043107 s\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed8137d3-f94c-450d-b8b1-0fa9e536b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "df = df.withColumn(\"u-g\", format_number(df[\"u\"]-df[\"g\"], 4).cast(\"double\"))\n",
    "df = df.withColumn(\"g-r\", format_number(df[\"g\"]-df[\"r\"], 4).cast(\"double\"))\n",
    "df = df.withColumn(\"r-i\", format_number(df[\"r\"]-df[\"i\"], 4).cast(\"double\"))\n",
    "df = df.withColumn(\"i-z\", format_number(df[\"i\"]-df[\"z\"], 4).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1def4b2-4c13-4a7e-ac9f-b15ab867e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('u', 'g', 'r', 'i', 'z', 'u-g', 'g-r', 'r-i', 'i-z', 'redshift', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7655f04c-8194-4b2f-a3db-027a98300a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+------+------+------+------+------------+------+\n",
      "|       u|       g|       r|       i|       z|   u-g|   g-r|   r-i|   i-z|    redshift| class|\n",
      "+--------+--------+--------+--------+--------+------+------+------+------+------------+------+\n",
      "|19.34492|16.92385|15.77771|15.33381|15.12316|2.4211|1.1461|0.4439|0.2106|-5.516215E-6|  STAR|\n",
      "|24.65397|22.53888|20.80799|19.86182|19.34411|2.1151|1.7309|0.9462|0.5177|   0.4979803|GALAXY|\n",
      "|21.42899|20.91531|20.86941|20.80266|20.38503|0.5137|0.0459|0.0667|0.4176|    2.442472|   QSO|\n",
      "|22.83521|21.55918|21.22711|20.94139|20.87671| 1.276|0.3321|0.2857|0.0647|-9.738771E-4|  STAR|\n",
      "| 23.0417|22.13414| 20.4698|18.59738|17.57369|0.9076|1.6643|1.8724|1.0237|-6.033703E-5|  STAR|\n",
      "+--------+--------+--------+--------+--------+------+------+------+------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:49:53 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:49:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:49:53 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 199.8 KiB, free 433.9 MiB)\n",
      "25/12/28 17:49:53 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
      "25/12/28 17:49:53 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:53 INFO SparkContext: Created broadcast 24 from showString at <unknown>:0\n",
      "25/12/28 17:49:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:49:53 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Got job 14 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Final stage: ResultStage 19 (showString at <unknown>:0)\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[59] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:49:53 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 25.4 KiB, free 433.9 MiB)\n",
      "25/12/28 17:49:53 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 9.9 KiB, free 433.8 MiB)\n",
      "25/12/28 17:49:53 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on f30b651f709e:39427 (size: 9.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:53 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[59] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:49:53 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:49:53 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 56) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:53 INFO Executor: Running task 0.0 in stage 19.0 (TID 56)\n",
      "25/12/28 17:49:53 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:49:53 INFO Executor: Finished task 0.0 in stage 19.0 (TID 56). 2146 bytes result sent to driver\n",
      "25/12/28 17:49:53 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 56) in 31 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:49:53 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:49:53 INFO DAGScheduler: ResultStage 19 (showString at <unknown>:0) finished in 0.038 s\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:49:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "25/12/28 17:49:53 INFO DAGScheduler: Job 14 finished: showString at <unknown>:0, took 0.041526 s\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12fca0fd-2c63-492b-96ae-6c733126bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:49:54 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:49:54 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:49:54 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 199.8 KiB, free 433.6 MiB)\n",
      "25/12/28 17:49:54 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.6 MiB)\n",
      "25/12/28 17:49:54 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on f30b651f709e:39427 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:54 INFO SparkContext: Created broadcast 26 from jdbc at <unknown>:0\n",
      "25/12/28 17:49:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336712 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:49:54 INFO SparkContext: Starting job: jdbc at <unknown>:0\n",
      "25/12/28 17:49:54 INFO DAGScheduler: Got job 15 (jdbc at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:49:54 INFO DAGScheduler: Final stage: ResultStage 20 (jdbc at <unknown>:0)\n",
      "25/12/28 17:49:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:49:54 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:49:54 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[65] at jdbc at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:49:54 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 45.8 KiB, free 433.6 MiB)\n",
      "25/12/28 17:49:54 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 433.6 MiB)\n",
      "25/12/28 17:49:54 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on f30b651f709e:39427 (size: 19.2 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:54 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:49:54 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 20 (MapPartitionsRDD[65] at jdbc at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:49:54 INFO TaskSchedulerImpl: Adding task set 20.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 57) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 58) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 2.0 in stage 20.0 (TID 59) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 3.0 in stage 20.0 (TID 60) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 4.0 in stage 20.0 (TID 61) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 5.0 in stage 20.0 (TID 62) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 6.0 in stage 20.0 (TID 63) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO TaskSetManager: Starting task 7.0 in stage 20.0 (TID 64) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:54 INFO Executor: Running task 0.0 in stage 20.0 (TID 57)\n",
      "25/12/28 17:49:54 INFO Executor: Running task 1.0 in stage 20.0 (TID 58)\n",
      "25/12/28 17:49:54 INFO Executor: Running task 2.0 in stage 20.0 (TID 59)\n",
      "25/12/28 17:49:54 INFO Executor: Running task 3.0 in stage 20.0 (TID 60)\n",
      "25/12/28 17:49:54 INFO Executor: Running task 4.0 in stage 20.0 (TID 61)\n",
      "25/12/28 17:49:54 INFO Executor: Running task 5.0 in stage 20.0 (TID 62)\n",
      "25/12/28 17:49:54 INFO Executor: Running task 6.0 in stage 20.0 (TID 63)\n",
      "25/12/28 17:49:54 INFO Executor: Running task 7.0 in stage 20.0 (TID 64)\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 4336712-8673424, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 13010136-17346848, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 8673424-13010136, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 0-4336712, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 17346848-21683560, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 21683560-26020272, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 30356984-30499392, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data2.csv, range: 26020272-30356984, partition values: [empty row]\n",
      "25/12/28 17:49:54 INFO Executor: Finished task 7.0 in stage 20.0 (TID 64). 1549 bytes result sent to driver\n",
      "25/12/28 17:49:54 INFO TaskSetManager: Finished task 7.0 in stage 20.0 (TID 64) in 435 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:49:54 INFO BlockManagerInfo: Removed broadcast_25_piece0 on f30b651f709e:39427 in memory (size: 9.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:54 INFO BlockManagerInfo: Removed broadcast_24_piece0 on f30b651f709e:39427 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:55 INFO BlockManagerInfo: Removed broadcast_23_piece0 on f30b651f709e:39427 in memory (size: 19.2 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:55 INFO BlockManagerInfo: Removed broadcast_22_piece0 on f30b651f709e:39427 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:50:04 INFO Executor: Finished task 6.0 in stage 20.0 (TID 63). 1592 bytes result sent to driver\n",
      "25/12/28 17:50:04 INFO TaskSetManager: Finished task 6.0 in stage 20.0 (TID 63) in 10221 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:50:04 INFO Executor: Finished task 1.0 in stage 20.0 (TID 58). 1592 bytes result sent to driver\n",
      "25/12/28 17:50:04 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 58) in 10241 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:50:04 INFO Executor: Finished task 5.0 in stage 20.0 (TID 62). 1592 bytes result sent to driver\n",
      "25/12/28 17:50:04 INFO TaskSetManager: Finished task 5.0 in stage 20.0 (TID 62) in 10336 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:50:04 INFO Executor: Finished task 4.0 in stage 20.0 (TID 61). 1592 bytes result sent to driver\n",
      "25/12/28 17:50:04 INFO Executor: Finished task 2.0 in stage 20.0 (TID 59). 1592 bytes result sent to driver\n",
      "25/12/28 17:50:04 INFO TaskSetManager: Finished task 4.0 in stage 20.0 (TID 61) in 10352 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:50:04 INFO TaskSetManager: Finished task 2.0 in stage 20.0 (TID 59) in 10353 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:50:04 INFO Executor: Finished task 3.0 in stage 20.0 (TID 60). 1592 bytes result sent to driver\n",
      "25/12/28 17:50:04 INFO TaskSetManager: Finished task 3.0 in stage 20.0 (TID 60) in 10358 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:50:04 INFO Executor: Finished task 0.0 in stage 20.0 (TID 57). 1592 bytes result sent to driver\n",
      "25/12/28 17:50:04 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 57) in 10415 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:50:04 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:50:04 INFO DAGScheduler: ResultStage 20 (jdbc at <unknown>:0) finished in 10.425 s\n",
      "25/12/28 17:50:04 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:50:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "25/12/28 17:50:04 INFO DAGScheduler: Job 15 finished: jdbc at <unknown>:0, took 10.436321 s\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.jdbc(\n",
    "    url=\"jdbc:postgresql://host.docker.internal:5432/postgres\",\n",
    "    table=\"processed_table\",\n",
    "    mode=\"append\",\n",
    "    properties={\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31dcbb-e6a3-4a5b-b927-c60e56a847fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
