{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7f78b0-d7fd-4922-84a1-a2c45851bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/28 17:40:15 WARN DependencyUtils: Local jar /opt/spark/postgresql-42.7.8.jar does not exist, skipping.\n",
      "25/12/28 17:40:15 INFO SparkContext: Running Spark version 3.4.0\n",
      "25/12/28 17:40:15 INFO ResourceUtils: ==============================================================\n",
      "25/12/28 17:40:15 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/12/28 17:40:15 INFO ResourceUtils: ==============================================================\n",
      "25/12/28 17:40:15 INFO SparkContext: Submitted application: pyspark-shell\n",
      "25/12/28 17:40:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/12/28 17:40:15 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/12/28 17:40:15 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/12/28 17:40:15 INFO SecurityManager: Changing view acls to: root\n",
      "25/12/28 17:40:15 INFO SecurityManager: Changing modify acls to: root\n",
      "25/12/28 17:40:15 INFO SecurityManager: Changing view acls groups to: \n",
      "25/12/28 17:40:15 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/12/28 17:40:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/12/28 17:40:16 INFO Utils: Successfully started service 'sparkDriver' on port 38157.\n",
      "25/12/28 17:40:16 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/12/28 17:40:16 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/12/28 17:40:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/12/28 17:40:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/12/28 17:40:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/12/28 17:40:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3f030abe-37cb-41a5-9fc4-1e3e442ccf94\n",
      "25/12/28 17:40:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/12/28 17:40:16 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/12/28 17:40:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/12/28 17:40:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/12/28 17:40:16 ERROR SparkContext: Failed to add /opt/spark/postgresql-42.7.8.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/spark/postgresql-42.7.8.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1968)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2023)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/12/28 17:40:16 INFO Executor: Starting executor ID driver on host f30b651f709e\n",
      "25/12/28 17:40:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/12/28 17:40:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36573.\n",
      "25/12/28 17:40:16 INFO NettyBlockTransferService: Server created on f30b651f709e:36573\n",
      "25/12/28 17:40:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/12/28 17:40:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f30b651f709e, 36573, None)\n",
      "25/12/28 17:40:16 INFO BlockManagerMasterEndpoint: Registering block manager f30b651f709e:36573 with 434.4 MiB RAM, BlockManagerId(driver, f30b651f709e, 36573, None)\n",
      "25/12/28 17:40:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f30b651f709e, 36573, None)\n",
      "25/12/28 17:40:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f30b651f709e, 36573, None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/postgresql-42.7.8.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac864d3-33a1-4cad-a843-d2066936d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/12/28 17:40:17 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.\n",
      "25/12/28 17:40:19 INFO InMemoryFileIndex: It took 70 ms to list leaf files for 1 paths.\n",
      "25/12/28 17:40:21 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:40:21 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:40:22 INFO CodeGenerator: Code generated in 240.491181 ms\n",
      "25/12/28 17:40:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.8 KiB, free 434.2 MiB)\n",
      "25/12/28 17:40:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)\n",
      "25/12/28 17:40:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:40:22 INFO SparkContext: Created broadcast 0 from showString at <unknown>:0\n",
      "25/12/28 17:40:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:40:22 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Got job 0 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Final stage: ResultStage 0 (showString at <unknown>:0)\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.9 KiB, free 434.2 MiB)\n",
      "25/12/28 17:40:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)\n",
      "25/12/28 17:40:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f30b651f709e:36573 (size: 7.8 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:40:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:40:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:40:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:40:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/12/28 17:40:23 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n",
      "25/12/28 17:40:23 INFO CodeGenerator: Code generated in 37.797729 ms\n",
      "25/12/28 17:40:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2051 bytes result sent to driver\n",
      "25/12/28 17:40:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 416 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:40:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:23 INFO DAGScheduler: ResultStage 0 (showString at <unknown>:0) finished in 0.617 s\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:40:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/12/28 17:40:23 INFO DAGScheduler: Job 0 finished: showString at <unknown>:0, took 0.681248 s\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+---------+------+\n",
      "|       u|       g|       r|       i|       z| redshift| class|\n",
      "+--------+--------+--------+--------+--------+---------+------+\n",
      "|23.52803|21.48264|19.68275|18.96128|18.44759| 0.399651|GALAXY|\n",
      "|21.62662|19.60713| 18.0404|17.44492|17.06376|0.2967839|GALAXY|\n",
      "|24.59877|21.30725| 19.6437| 19.0284| 18.6411|0.3316356|GALAXY|\n",
      "| 26.7289|21.58765|19.89412|19.04494|18.47363|0.4946944|GALAXY|\n",
      "|23.31986|21.93849|20.30019|19.27454|18.82292|0.5611351|GALAXY|\n",
      "+--------+--------+--------+--------+--------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:23 INFO CodeGenerator: Code generated in 39.622413 ms\n"
     ]
    }
   ],
   "source": [
    "schema_ddl = \"u DOUBLE, g DOUBLE, r DOUBLE, i DOUBLE, z DOUBLE, redshift DOUBLE, class STRING\"\n",
    "df = spark.read.csv(\"raw_data1.csv\", header=True, schema=schema_ddl)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a3cf87-882f-4fad-9936-202bae9c1701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:25 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:40:25 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:40:25 INFO CodeGenerator: Code generated in 30.881265 ms\n",
      "25/12/28 17:40:25 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)\n",
      "25/12/28 17:40:25 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
      "25/12/28 17:40:25 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:25 INFO SparkContext: Created broadcast 2 from showString at <unknown>:0\n",
      "25/12/28 17:40:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:40:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on f30b651f709e:36573 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:25 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Got job 1 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Final stage: ResultStage 1 (showString at <unknown>:0)\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:25 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.9 KiB, free 433.9 MiB)\n",
      "25/12/28 17:40:25 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)\n",
      "25/12/28 17:40:25 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on f30b651f709e:36573 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:25 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:40:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:40:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:40:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/12/28 17:40:25 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+------+---------+\n",
      "|       u|       g|       r|       i|       z| class| redshift|\n",
      "+--------+--------+--------+--------+--------+------+---------+\n",
      "|23.52803|21.48264|19.68275|18.96128|18.44759|GALAXY| 0.399651|\n",
      "|21.62662|19.60713| 18.0404|17.44492|17.06376|GALAXY|0.2967839|\n",
      "|24.59877|21.30725| 19.6437| 19.0284| 18.6411|GALAXY|0.3316356|\n",
      "| 26.7289|21.58765|19.89412|19.04494|18.47363|GALAXY|0.4946944|\n",
      "|23.31986|21.93849|20.30019|19.27454|18.82292|GALAXY|0.5611351|\n",
      "+--------+--------+--------+--------+--------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:25 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver\n",
      "25/12/28 17:40:25 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 77 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:40:25 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:25 INFO DAGScheduler: ResultStage 1 (showString at <unknown>:0) finished in 0.096 s\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:40:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/12/28 17:40:25 INFO DAGScheduler: Job 1 finished: showString at <unknown>:0, took 0.104966 s\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"u\", \"g\", \"r\", \"i\", \"z\", \"class\", \"redshift\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70c44a6c-fdc5-40ce-bf90-f751787a8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- u: double (nullable = true)\n",
      " |-- g: double (nullable = true)\n",
      " |-- r: double (nullable = true)\n",
      " |-- i: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- redshift: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6aafe6-0145-4525-b1bf-ac49ae8b6d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:27 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:40:27 INFO FileSourceStrategy: Post-Scan Filters: atleastnnonnulls(7, u#0, g#1, r#2, i#3, z#4, class#6, redshift#5)\n",
      "25/12/28 17:40:27 INFO CodeGenerator: Code generated in 28.921876 ms\n",
      "25/12/28 17:40:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.8 KiB, free 433.7 MiB)\n",
      "25/12/28 17:40:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.7 MiB)\n",
      "25/12/28 17:40:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:27 INFO SparkContext: Created broadcast 4 from count at <unknown>:0\n",
      "25/12/28 17:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:40:27 INFO DAGScheduler: Registering RDD 11 (count at <unknown>:0) as input to shuffle 0\n",
      "25/12/28 17:40:27 INFO DAGScheduler: Got map stage job 2 (count at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:40:27 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at <unknown>:0)\n",
      "25/12/28 17:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:40:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:27 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[11] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 19.6 KiB, free 433.7 MiB)\n",
      "25/12/28 17:40:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 433.7 MiB)\n",
      "25/12/28 17:40:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on f30b651f709e:36573 (size: 9.2 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:27 INFO BlockManagerInfo: Removed broadcast_2_piece0 on f30b651f709e:36573 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:27 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[11] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:40:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO BlockManagerInfo: Removed broadcast_3_piece0 on f30b651f709e:36573 in memory (size: 7.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 4) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 5) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 6) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 7) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 8) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 9) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "25/12/28 17:40:27 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)\n",
      "25/12/28 17:40:27 INFO Executor: Running task 2.0 in stage 2.0 (TID 4)\n",
      "25/12/28 17:40:27 INFO Executor: Running task 3.0 in stage 2.0 (TID 5)\n",
      "25/12/28 17:40:27 INFO Executor: Running task 4.0 in stage 2.0 (TID 6)\n",
      "25/12/28 17:40:27 INFO Executor: Running task 5.0 in stage 2.0 (TID 7)\n",
      "25/12/28 17:40:27 INFO Executor: Running task 6.0 in stage 2.0 (TID 8)\n",
      "25/12/28 17:40:27 INFO Executor: Running task 7.0 in stage 2.0 (TID 9)\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 21684205-26021046, partition values: [empty row]\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 30357887-30500425, partition values: [empty row]\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 13010523-17347364, partition values: [empty row]\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 4336841-8673682, partition values: [empty row]\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 8673682-13010523, partition values: [empty row]\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 26021046-30357887, partition values: [empty row]\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 17347364-21684205, partition values: [empty row]\n",
      "25/12/28 17:40:27 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n",
      "25/12/28 17:40:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f30b651f709e:36573 in memory (size: 34.3 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 7.0 in stage 2.0 (TID 9). 2070 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 9) in 389 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2027 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 951 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 6.0 in stage 2.0 (TID 8). 2027 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 8) in 969 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 2.0 in stage 2.0 (TID 4). 2027 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 4.0 in stage 2.0 (TID 6). 2027 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 4) in 1007 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 6) in 1005 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 2027 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 5.0 in stage 2.0 (TID 7). 2027 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO Executor: Finished task 3.0 in stage 2.0 (TID 5). 2027 bytes result sent to driver\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 1015 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 5) in 1015 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 7) in 1013 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:40:28 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:28 INFO DAGScheduler: ShuffleMapStage 2 (count at <unknown>:0) finished in 1.080 s\n",
      "25/12/28 17:40:28 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:40:28 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:40:28 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:40:28 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:40:28 INFO CodeGenerator: Code generated in 15.840002 ms\n",
      "25/12/28 17:40:28 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "25/12/28 17:40:28 INFO DAGScheduler: Got job 3 (count at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:40:28 INFO DAGScheduler: Final stage: ResultStage 4 (count at <unknown>:0)\n",
      "25/12/28 17:40:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\n",
      "25/12/28 17:40:28 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:28 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:28 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 12.1 KiB, free 434.1 MiB)\n",
      "25/12/28 17:40:28 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.1 MiB)\n",
      "25/12/28 17:40:28 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on f30b651f709e:36573 (size: 5.8 KiB, free: 434.4 MiB)\n",
      "25/12/28 17:40:28 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:40:28 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:40:28 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 10) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "25/12/28 17:40:28 INFO Executor: Running task 0.0 in stage 4.0 (TID 10)\n",
      "25/12/28 17:40:29 INFO ShuffleBlockFetcherIterator: Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms\n",
      "25/12/28 17:40:29 INFO Executor: Finished task 0.0 in stage 4.0 (TID 10). 3995 bytes result sent to driver\n",
      "25/12/28 17:40:29 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 10) in 84 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:40:29 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:29 INFO DAGScheduler: ResultStage 4 (count at <unknown>:0) finished in 0.102 s\n",
      "25/12/28 17:40:29 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:40:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "25/12/28 17:40:29 INFO DAGScheduler: Job 3 finished: count at <unknown>:0, took 0.124847 s\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(subset=df.columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29fd08a-4d9b-4b15-96ef-a6a40c5b42c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:30 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:40:30 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:40:30 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/12/28 17:40:30 INFO CodeGenerator: Code generated in 45.719726 ms\n",
      "25/12/28 17:40:30 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 199.8 KiB, free 433.9 MiB)\n",
      "25/12/28 17:40:30 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
      "25/12/28 17:40:30 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:30 INFO SparkContext: Created broadcast 7 from count at <unknown>:0\n",
      "25/12/28 17:40:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:40:30 INFO DAGScheduler: Registering RDD 18 (count at <unknown>:0) as input to shuffle 1\n",
      "25/12/28 17:40:30 INFO DAGScheduler: Got map stage job 4 (count at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:40:30 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (count at <unknown>:0)\n",
      "25/12/28 17:40:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:40:30 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:30 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[18] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:30 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 38.4 KiB, free 433.9 MiB)\n",
      "25/12/28 17:40:30 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 433.8 MiB)\n",
      "25/12/28 17:40:30 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on f30b651f709e:36573 (size: 17.1 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:30 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:30 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[18] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:40:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 12) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 13) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 14) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 15) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 16) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 17) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 18) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:30 INFO Executor: Running task 2.0 in stage 5.0 (TID 13)\n",
      "25/12/28 17:40:30 INFO Executor: Running task 1.0 in stage 5.0 (TID 12)\n",
      "25/12/28 17:40:30 INFO Executor: Running task 4.0 in stage 5.0 (TID 15)\n",
      "25/12/28 17:40:30 INFO Executor: Running task 3.0 in stage 5.0 (TID 14)\n",
      "25/12/28 17:40:30 INFO Executor: Running task 6.0 in stage 5.0 (TID 17)\n",
      "25/12/28 17:40:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 11)\n",
      "25/12/28 17:40:30 INFO Executor: Running task 5.0 in stage 5.0 (TID 16)\n",
      "25/12/28 17:40:30 INFO Executor: Running task 7.0 in stage 5.0 (TID 18)\n",
      "25/12/28 17:40:30 INFO CodeGenerator: Code generated in 31.331671 ms\n",
      "25/12/28 17:40:30 INFO CodeGenerator: Code generated in 9.297659 ms\n",
      "25/12/28 17:40:30 INFO CodeGenerator: Code generated in 16.410282 ms\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 4336841-8673682, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 8673682-13010523, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 26021046-30357887, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 13010523-17347364, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 17347364-21684205, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 30357887-30500425, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 21684205-26021046, partition values: [empty row]\n",
      "25/12/28 17:40:30 INFO BlockManagerInfo: Removed broadcast_6_piece0 on f30b651f709e:36573 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:30 INFO Executor: Finished task 7.0 in stage 5.0 (TID 18). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:30 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 18) in 595 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:40:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on f30b651f709e:36573 in memory (size: 9.2 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:31 INFO BlockManagerInfo: Removed broadcast_4_piece0 on f30b651f709e:36573 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 5.0 in stage 5.0 (TID 16). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 16) in 1837 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 1.0 in stage 5.0 (TID 12). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 12) in 1858 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 2.0 in stage 5.0 (TID 13). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 13) in 1877 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 3.0 in stage 5.0 (TID 14). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 14) in 1881 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 4.0 in stage 5.0 (TID 15). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 15) in 1888 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 0.0 in stage 5.0 (TID 11). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 1896 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 6.0 in stage 5.0 (TID 17). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 17) in 1895 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:40:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:32 INFO DAGScheduler: ShuffleMapStage 5 (count at <unknown>:0) finished in 1.916 s\n",
      "25/12/28 17:40:32 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:40:32 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:40:32 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:40:32 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:40:32 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 3289355, minimum partition size: 1048576\n",
      "25/12/28 17:40:32 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/12/28 17:40:32 INFO CodeGenerator: Code generated in 71.783577 ms\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Registering RDD 21 (count at <unknown>:0) as input to shuffle 2\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Got map stage job 5 (count at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (count at <unknown>:0)\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[21] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:32 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 45.9 KiB, free 434.1 MiB)\n",
      "25/12/28 17:40:32 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.9 KiB, free 434.1 MiB)\n",
      "25/12/28 17:40:32 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on f30b651f709e:36573 (size: 20.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:32 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[21] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:40:32 INFO TaskSchedulerImpl: Adding task set 7.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 19) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 20) (f30b651f709e, executor driver, partition 1, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 21) (f30b651f709e, executor driver, partition 2, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 22) (f30b651f709e, executor driver, partition 3, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 23) (f30b651f709e, executor driver, partition 4, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 24) (f30b651f709e, executor driver, partition 5, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 25) (f30b651f709e, executor driver, partition 6, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 26) (f30b651f709e, executor driver, partition 7, NODE_LOCAL, 7352 bytes) \n",
      "25/12/28 17:40:32 INFO Executor: Running task 1.0 in stage 7.0 (TID 20)\n",
      "25/12/28 17:40:32 INFO Executor: Running task 0.0 in stage 7.0 (TID 19)\n",
      "25/12/28 17:40:32 INFO Executor: Running task 4.0 in stage 7.0 (TID 23)\n",
      "25/12/28 17:40:32 INFO Executor: Running task 6.0 in stage 7.0 (TID 25)\n",
      "25/12/28 17:40:32 INFO Executor: Running task 2.0 in stage 7.0 (TID 21)\n",
      "25/12/28 17:40:32 INFO Executor: Running task 7.0 in stage 7.0 (TID 26)\n",
      "25/12/28 17:40:32 INFO Executor: Running task 5.0 in stage 7.0 (TID 24)\n",
      "25/12/28 17:40:32 INFO Executor: Running task 3.0 in stage 7.0 (TID 22)\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.0 MiB) non-empty blocks including 8 (3.0 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.5 MiB) non-empty blocks including 8 (3.5 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Getting 8 (3.1 MiB) non-empty blocks including 8 (3.1 MiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 4.0 in stage 7.0 (TID 23). 5595 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 3.0 in stage 7.0 (TID 22). 5595 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 23) in 291 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 22) in 301 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 0.0 in stage 7.0 (TID 19). 5638 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 5.0 in stage 7.0 (TID 24). 5638 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 19) in 332 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 6.0 in stage 7.0 (TID 25). 5638 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 24) in 334 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 25) in 334 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 2.0 in stage 7.0 (TID 21). 5638 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 21) in 344 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 1.0 in stage 7.0 (TID 20). 5638 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 20) in 359 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:40:32 INFO Executor: Finished task 7.0 in stage 7.0 (TID 26). 5638 bytes result sent to driver\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 26) in 370 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:40:32 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:32 INFO DAGScheduler: ShuffleMapStage 7 (count at <unknown>:0) finished in 0.393 s\n",
      "25/12/28 17:40:32 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:40:32 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:40:32 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:40:32 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:40:32 INFO CodeGenerator: Code generated in 10.144628 ms\n",
      "25/12/28 17:40:32 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Got job 6 (count at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Final stage: ResultStage 10 (count at <unknown>:0)\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[24] at count at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:32 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.1 KiB, free 434.0 MiB)\n",
      "25/12/28 17:40:32 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.0 MiB)\n",
      "25/12/28 17:40:32 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on f30b651f709e:36573 (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:32 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[24] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:40:32 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:40:32 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 27) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "25/12/28 17:40:33 INFO Executor: Running task 0.0 in stage 10.0 (TID 27)\n",
      "25/12/28 17:40:33 INFO ShuffleBlockFetcherIterator: Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/12/28 17:40:33 INFO Executor: Finished task 0.0 in stage 10.0 (TID 27). 3995 bytes result sent to driver\n",
      "25/12/28 17:40:33 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 27) in 15 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:40:33 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:33 INFO DAGScheduler: ResultStage 10 (count at <unknown>:0) finished in 0.026 s\n",
      "25/12/28 17:40:33 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:40:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "25/12/28 17:40:33 INFO DAGScheduler: Job 6 finished: count at <unknown>:0, took 0.033429 s\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0053e2d1-af42-4e2f-948d-428a6d72a18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[u: double, g: double, r: double, i: double, z: double, redshift: double, class: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['u', 'g', 'r', 'i', 'z', 'redshift', 'class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d764c1ed-fff2-4b95-ac28-c413171a3e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:34 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:40:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:40:34 INFO CodeGenerator: Code generated in 50.507851 ms\n",
      "25/12/28 17:40:34 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 199.8 KiB, free 433.8 MiB)\n",
      "25/12/28 17:40:34 INFO BlockManagerInfo: Removed broadcast_10_piece0 on f30b651f709e:36573 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:34 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.8 MiB)\n",
      "25/12/28 17:40:34 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:34 INFO SparkContext: Created broadcast 11 from showString at <unknown>:0\n",
      "25/12/28 17:40:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:40:34 INFO DAGScheduler: Registering RDD 28 (showString at <unknown>:0) as input to shuffle 3\n",
      "25/12/28 17:40:34 INFO DAGScheduler: Got map stage job 7 (showString at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:40:34 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (showString at <unknown>:0)\n",
      "25/12/28 17:40:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:40:34 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:34 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[28] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:34 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 37.1 KiB, free 433.8 MiB)\n",
      "25/12/28 17:40:34 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 433.8 MiB)\n",
      "25/12/28 17:40:34 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on f30b651f709e:36573 (size: 17.7 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:34 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:34 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[28] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:40:34 INFO TaskSchedulerImpl: Adding task set 11.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 28) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 29) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 30) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO BlockManagerInfo: Removed broadcast_8_piece0 on f30b651f709e:36573 in memory (size: 17.1 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 3.0 in stage 11.0 (TID 31) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 4.0 in stage 11.0 (TID 32) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 5.0 in stage 11.0 (TID 33) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 6.0 in stage 11.0 (TID 34) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO TaskSetManager: Starting task 7.0 in stage 11.0 (TID 35) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7909 bytes) \n",
      "25/12/28 17:40:34 INFO Executor: Running task 0.0 in stage 11.0 (TID 28)\n",
      "25/12/28 17:40:34 INFO Executor: Running task 2.0 in stage 11.0 (TID 30)\n",
      "25/12/28 17:40:34 INFO Executor: Running task 6.0 in stage 11.0 (TID 34)\n",
      "25/12/28 17:40:34 INFO Executor: Running task 7.0 in stage 11.0 (TID 35)\n",
      "25/12/28 17:40:34 INFO Executor: Running task 3.0 in stage 11.0 (TID 31)\n",
      "25/12/28 17:40:34 INFO Executor: Running task 1.0 in stage 11.0 (TID 29)\n",
      "25/12/28 17:40:34 INFO BlockManagerInfo: Removed broadcast_9_piece0 on f30b651f709e:36573 in memory (size: 20.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:34 INFO Executor: Running task 4.0 in stage 11.0 (TID 32)\n",
      "25/12/28 17:40:34 INFO Executor: Running task 5.0 in stage 11.0 (TID 33)\n",
      "25/12/28 17:40:35 INFO CodeGenerator: Code generated in 12.666079 ms\n",
      "25/12/28 17:40:35 INFO CodeGenerator: Code generated in 9.949192 ms\n",
      "25/12/28 17:40:35 INFO CodeGenerator: Code generated in 9.172846 ms\n",
      "25/12/28 17:40:35 INFO CodeGenerator: Code generated in 6.441395 ms\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 30357887-30500425, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 13010523-17347364, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 26021046-30357887, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 21684205-26021046, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 4336841-8673682, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 8673682-13010523, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 17347364-21684205, partition values: [empty row]\n",
      "25/12/28 17:40:35 INFO Executor: Finished task 7.0 in stage 11.0 (TID 35). 2722 bytes result sent to driver\n",
      "25/12/28 17:40:35 INFO TaskSetManager: Finished task 7.0 in stage 11.0 (TID 35) in 179 ms on f30b651f709e (executor driver) (1/8)\n",
      "[Stage 11:=======>                                                  (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| class| count|\n",
      "+------+------+\n",
      "|GALAXY|302694|\n",
      "|   QSO| 93491|\n",
      "|  STAR|103815|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:36 INFO Executor: Finished task 3.0 in stage 11.0 (TID 31). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 3.0 in stage 11.0 (TID 31) in 1360 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:40:36 INFO Executor: Finished task 5.0 in stage 11.0 (TID 33). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 5.0 in stage 11.0 (TID 33) in 1368 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:40:36 INFO Executor: Finished task 2.0 in stage 11.0 (TID 30). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 30) in 1384 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:40:36 INFO Executor: Finished task 6.0 in stage 11.0 (TID 34). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO Executor: Finished task 1.0 in stage 11.0 (TID 29). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 6.0 in stage 11.0 (TID 34) in 1381 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:40:36 INFO Executor: Finished task 4.0 in stage 11.0 (TID 32). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 29) in 1389 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 4.0 in stage 11.0 (TID 32) in 1386 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:40:36 INFO Executor: Finished task 0.0 in stage 11.0 (TID 28). 2765 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 28) in 1402 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:40:36 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:36 INFO DAGScheduler: ShuffleMapStage 11 (showString at <unknown>:0) finished in 1.413 s\n",
      "25/12/28 17:40:36 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/12/28 17:40:36 INFO DAGScheduler: running: Set()\n",
      "25/12/28 17:40:36 INFO DAGScheduler: waiting: Set()\n",
      "25/12/28 17:40:36 INFO DAGScheduler: failed: Set()\n",
      "25/12/28 17:40:36 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/12/28 17:40:36 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/12/28 17:40:36 INFO CodeGenerator: Code generated in 16.498654 ms\n",
      "25/12/28 17:40:36 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Got job 8 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Final stage: ResultStage 13 (showString at <unknown>:0)\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[31] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:36 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 38.9 KiB, free 433.9 MiB)\n",
      "25/12/28 17:40:36 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 18.9 KiB, free 433.8 MiB)\n",
      "25/12/28 17:40:36 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on f30b651f709e:36573 (size: 18.9 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:36 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[31] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:40:36 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 36) (f30b651f709e, executor driver, partition 0, NODE_LOCAL, 7363 bytes) \n",
      "25/12/28 17:40:36 INFO Executor: Running task 0.0 in stage 13.0 (TID 36)\n",
      "25/12/28 17:40:36 INFO ShuffleBlockFetcherIterator: Getting 8 (1728.0 B) non-empty blocks including 8 (1728.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/12/28 17:40:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/12/28 17:40:36 INFO Executor: Finished task 0.0 in stage 13.0 (TID 36). 5154 bytes result sent to driver\n",
      "25/12/28 17:40:36 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 36) in 21 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:40:36 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:36 INFO DAGScheduler: ResultStage 13 (showString at <unknown>:0) finished in 0.032 s\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:40:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "25/12/28 17:40:36 INFO DAGScheduler: Job 8 finished: showString at <unknown>:0, took 0.040403 s\n",
      "25/12/28 17:40:36 INFO CodeGenerator: Code generated in 7.439372 ms             \n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30ae81c-8d14-4119-8afe-9b828e4477ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+------+---------+\n",
      "|       u|       g|       r|       i|       z| class| redshift|\n",
      "+--------+--------+--------+--------+--------+------+---------+\n",
      "|23.52803|21.48264|19.68275|18.96128|18.44759|GALAXY| 0.399651|\n",
      "|21.62662|19.60713| 18.0404|17.44492|17.06376|GALAXY|0.2967839|\n",
      "|24.59877|21.30725| 19.6437| 19.0284| 18.6411|GALAXY|0.3316356|\n",
      "| 26.7289|21.58765|19.89412|19.04494|18.47363|GALAXY|0.4946944|\n",
      "|23.31986|21.93849|20.30019|19.27454|18.82292|GALAXY|0.5611351|\n",
      "+--------+--------+--------+--------+--------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:40:37 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:40:37 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:40:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 199.8 KiB, free 433.6 MiB)\n",
      "25/12/28 17:40:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.6 MiB)\n",
      "25/12/28 17:40:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:37 INFO SparkContext: Created broadcast 14 from showString at <unknown>:0\n",
      "25/12/28 17:40:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:40:37 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Got job 9 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Final stage: ResultStage 14 (showString at <unknown>:0)\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[35] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:40:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 16.9 KiB, free 433.6 MiB)\n",
      "25/12/28 17:40:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.6 MiB)\n",
      "25/12/28 17:40:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on f30b651f709e:36573 (size: 7.8 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:40:37 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[35] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:40:37 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:40:37 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 37) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:40:37 INFO Executor: Running task 0.0 in stage 14.0 (TID 37)\n",
      "25/12/28 17:40:37 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n",
      "25/12/28 17:40:37 INFO Executor: Finished task 0.0 in stage 14.0 (TID 37). 1964 bytes result sent to driver\n",
      "25/12/28 17:40:37 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 37) in 29 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:40:37 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:40:37 INFO DAGScheduler: ResultStage 14 (showString at <unknown>:0) finished in 0.037 s\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:40:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "25/12/28 17:40:37 INFO DAGScheduler: Job 9 finished: showString at <unknown>:0, took 0.041783 s\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed8137d3-f94c-450d-b8b1-0fa9e536b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "df = df.withColumn(\"u-g\", format_number(df[\"u\"]-df[\"g\"], 4).cast(\"double\"))\n",
    "df = df.withColumn(\"g-r\", format_number(df[\"g\"]-df[\"r\"], 4).cast(\"double\"))\n",
    "df = df.withColumn(\"r-i\", format_number(df[\"r\"]-df[\"i\"], 4).cast(\"double\"))\n",
    "df = df.withColumn(\"i-z\", format_number(df[\"i\"]-df[\"z\"], 4).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1def4b2-4c13-4a7e-ac9f-b15ab867e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('u', 'g', 'r', 'i', 'z', 'u-g', 'g-r', 'r-i', 'i-z', 'redshift', 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7655f04c-8194-4b2f-a3db-027a98300a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+------+------+------+------+---------+------+\n",
      "|       u|       g|       r|       i|       z|   u-g|   g-r|   r-i|   i-z| redshift| class|\n",
      "+--------+--------+--------+--------+--------+------+------+------+------+---------+------+\n",
      "|23.52803|21.48264|19.68275|18.96128|18.44759|2.0454|1.7999|0.7215|0.5137| 0.399651|GALAXY|\n",
      "|21.62662|19.60713| 18.0404|17.44492|17.06376|2.0195|1.5667|0.5955|0.3812|0.2967839|GALAXY|\n",
      "|24.59877|21.30725| 19.6437| 19.0284| 18.6411|3.2915|1.6636|0.6153|0.3873|0.3316356|GALAXY|\n",
      "| 26.7289|21.58765|19.89412|19.04494|18.47363|5.1412|1.6935|0.8492|0.5713|0.4946944|GALAXY|\n",
      "|23.31986|21.93849|20.30019|19.27454|18.82292|1.3814|1.6383|1.0257|0.4516|0.5611351|GALAXY|\n",
      "+--------+--------+--------+--------+--------+------+------+------+------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:49:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:49:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:49:38 INFO CodeGenerator: Code generated in 34.41572 ms\n",
      "25/12/28 17:49:38 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 199.8 KiB, free 433.9 MiB)\n",
      "25/12/28 17:49:38 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)\n",
      "25/12/28 17:49:38 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:38 INFO SparkContext: Created broadcast 20 from showString at <unknown>:0\n",
      "25/12/28 17:49:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:49:38 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Got job 12 (showString at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Final stage: ResultStage 17 (showString at <unknown>:0)\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[49] at showString at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:49:38 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 25.4 KiB, free 433.9 MiB)\n",
      "25/12/28 17:49:38 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 10.0 KiB, free 433.8 MiB)\n",
      "25/12/28 17:49:38 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on f30b651f709e:36573 (size: 10.0 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:38 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[49] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:49:38 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:49:38 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 47) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:38 INFO Executor: Running task 0.0 in stage 17.0 (TID 47)\n",
      "25/12/28 17:49:38 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n",
      "25/12/28 17:49:38 INFO Executor: Finished task 0.0 in stage 17.0 (TID 47). 2148 bytes result sent to driver\n",
      "25/12/28 17:49:38 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 47) in 36 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:49:38 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:49:38 INFO DAGScheduler: ResultStage 17 (showString at <unknown>:0) finished in 0.048 s\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:49:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "25/12/28 17:49:38 INFO DAGScheduler: Job 12 finished: showString at <unknown>:0, took 0.053184 s\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12fca0fd-2c63-492b-96ae-6c733126bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:49:39 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/28 17:49:39 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/28 17:49:39 INFO CodeGenerator: Code generated in 24.230529 ms\n",
      "25/12/28 17:49:39 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 199.8 KiB, free 433.7 MiB)\n",
      "25/12/28 17:49:39 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.6 MiB)\n",
      "25/12/28 17:49:39 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on f30b651f709e:36573 (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:39 INFO SparkContext: Created broadcast 22 from jdbc at <unknown>:0\n",
      "25/12/28 17:49:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4336841 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/28 17:49:39 INFO SparkContext: Starting job: jdbc at <unknown>:0\n",
      "25/12/28 17:49:39 INFO DAGScheduler: Got job 13 (jdbc at <unknown>:0) with 8 output partitions\n",
      "25/12/28 17:49:39 INFO DAGScheduler: Final stage: ResultStage 18 (jdbc at <unknown>:0)\n",
      "25/12/28 17:49:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:49:39 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:49:39 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[55] at jdbc at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:49:39 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 45.8 KiB, free 433.6 MiB)\n",
      "25/12/28 17:49:39 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 433.6 MiB)\n",
      "25/12/28 17:49:39 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on f30b651f709e:36573 (size: 19.2 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:39 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:49:39 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 18 (MapPartitionsRDD[55] at jdbc at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "25/12/28 17:49:39 INFO TaskSchedulerImpl: Adding task set 18.0 with 8 tasks resource profile 0\n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 48) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 49) (f30b651f709e, executor driver, partition 1, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 2.0 in stage 18.0 (TID 50) (f30b651f709e, executor driver, partition 2, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 3.0 in stage 18.0 (TID 51) (f30b651f709e, executor driver, partition 3, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 4.0 in stage 18.0 (TID 52) (f30b651f709e, executor driver, partition 4, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 5.0 in stage 18.0 (TID 53) (f30b651f709e, executor driver, partition 5, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 6.0 in stage 18.0 (TID 54) (f30b651f709e, executor driver, partition 6, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO TaskSetManager: Starting task 7.0 in stage 18.0 (TID 55) (f30b651f709e, executor driver, partition 7, PROCESS_LOCAL, 7920 bytes) \n",
      "25/12/28 17:49:39 INFO Executor: Running task 0.0 in stage 18.0 (TID 48)\n",
      "25/12/28 17:49:39 INFO Executor: Running task 1.0 in stage 18.0 (TID 49)\n",
      "25/12/28 17:49:39 INFO Executor: Running task 2.0 in stage 18.0 (TID 50)\n",
      "25/12/28 17:49:39 INFO Executor: Running task 3.0 in stage 18.0 (TID 51)\n",
      "25/12/28 17:49:39 INFO Executor: Running task 4.0 in stage 18.0 (TID 52)\n",
      "25/12/28 17:49:39 INFO Executor: Running task 5.0 in stage 18.0 (TID 53)\n",
      "25/12/28 17:49:39 INFO Executor: Running task 6.0 in stage 18.0 (TID 54)\n",
      "25/12/28 17:49:39 INFO Executor: Running task 7.0 in stage 18.0 (TID 55)\n",
      "25/12/28 17:49:39 INFO CodeGenerator: Code generated in 16.861265 ms\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 13010523-17347364, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 0-4336841, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 17347364-21684205, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 21684205-26021046, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 30357887-30500425, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 8673682-13010523, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 4336841-8673682, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/raw_data1.csv, range: 26021046-30357887, partition values: [empty row]\n",
      "25/12/28 17:49:39 INFO Executor: Finished task 7.0 in stage 18.0 (TID 55). 1549 bytes result sent to driver\n",
      "25/12/28 17:49:39 INFO TaskSetManager: Finished task 7.0 in stage 18.0 (TID 55) in 462 ms on f30b651f709e (executor driver) (1/8)\n",
      "25/12/28 17:49:39 INFO BlockManagerInfo: Removed broadcast_21_piece0 on f30b651f709e:36573 in memory (size: 10.0 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:39 INFO BlockManagerInfo: Removed broadcast_20_piece0 on f30b651f709e:36573 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:41 INFO BlockManagerInfo: Removed broadcast_18_piece0 on f30b651f709e:36573 in memory (size: 34.3 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:41 INFO BlockManagerInfo: Removed broadcast_19_piece0 on f30b651f709e:36573 in memory (size: 18.2 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:49:47 INFO Executor: Finished task 5.0 in stage 18.0 (TID 53). 1635 bytes result sent to driver\n",
      "25/12/28 17:49:47 INFO TaskSetManager: Finished task 5.0 in stage 18.0 (TID 53) in 7888 ms on f30b651f709e (executor driver) (2/8)\n",
      "25/12/28 17:49:47 INFO Executor: Finished task 2.0 in stage 18.0 (TID 50). 1592 bytes result sent to driver\n",
      "25/12/28 17:49:47 INFO TaskSetManager: Finished task 2.0 in stage 18.0 (TID 50) in 8009 ms on f30b651f709e (executor driver) (3/8)\n",
      "25/12/28 17:49:47 INFO Executor: Finished task 0.0 in stage 18.0 (TID 48). 1592 bytes result sent to driver\n",
      "25/12/28 17:49:47 INFO Executor: Finished task 3.0 in stage 18.0 (TID 51). 1592 bytes result sent to driver\n",
      "25/12/28 17:49:47 INFO Executor: Finished task 1.0 in stage 18.0 (TID 49). 1592 bytes result sent to driver\n",
      "25/12/28 17:49:47 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 48) in 8017 ms on f30b651f709e (executor driver) (4/8)\n",
      "25/12/28 17:49:47 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 49) in 8016 ms on f30b651f709e (executor driver) (5/8)\n",
      "25/12/28 17:49:47 INFO TaskSetManager: Finished task 3.0 in stage 18.0 (TID 51) in 8016 ms on f30b651f709e (executor driver) (6/8)\n",
      "25/12/28 17:49:47 INFO Executor: Finished task 4.0 in stage 18.0 (TID 52). 1592 bytes result sent to driver\n",
      "25/12/28 17:49:47 INFO TaskSetManager: Finished task 4.0 in stage 18.0 (TID 52) in 8021 ms on f30b651f709e (executor driver) (7/8)\n",
      "25/12/28 17:49:47 INFO Executor: Finished task 6.0 in stage 18.0 (TID 54). 1592 bytes result sent to driver\n",
      "25/12/28 17:49:47 INFO TaskSetManager: Finished task 6.0 in stage 18.0 (TID 54) in 8043 ms on f30b651f709e (executor driver) (8/8)\n",
      "25/12/28 17:49:47 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:49:47 INFO DAGScheduler: ResultStage 18 (jdbc at <unknown>:0) finished in 8.058 s\n",
      "25/12/28 17:49:47 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:49:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "25/12/28 17:49:47 INFO DAGScheduler: Job 13 finished: jdbc at <unknown>:0, took 8.062718 s\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.jdbc(\n",
    "    url=\"jdbc:postgresql://host.docker.internal:5432/postgres\",\n",
    "    table=\"processed_table\",\n",
    "    mode=\"overwrite\",\n",
    "    properties={\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31dcbb-e6a3-4a5b-b927-c60e56a847fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
