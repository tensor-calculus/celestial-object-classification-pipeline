{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9929e15e-104c-466d-9087-938d112ab26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/postgresql-42.7.8.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data from Postgres\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://host.docker.internal:5432/postgres\") \\\n",
    "    .option(\"dbtable\", \"processed_table\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2040e104-ce80-4c5e-befd-aa5c00def4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = {\"GALAXY\": 0.1, \"STAR\": 0.1, \"QSO\": 0.1}\n",
    "sampled_df = df.stat.sampleBy(\"class\", fractions, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc567cdd-8062-4382-9108-f0a81ed038d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/28 17:50:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/12/28 17:50:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/12/28 17:50:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "25/12/28 17:50:29 INFO CodeGenerator: Code generated in 43.449475 ms\n",
      "25/12/28 17:50:29 INFO SparkContext: Starting job: csv at <unknown>:0\n",
      "25/12/28 17:50:29 INFO DAGScheduler: Got job 1 (csv at <unknown>:0) with 1 output partitions\n",
      "25/12/28 17:50:29 INFO DAGScheduler: Final stage: ResultStage 1 (csv at <unknown>:0)\n",
      "25/12/28 17:50:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/28 17:50:29 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/28 17:50:29 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at csv at <unknown>:0), which has no missing parents\n",
      "25/12/28 17:50:30 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 231.1 KiB, free 433.9 MiB)\n",
      "25/12/28 17:50:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 84.6 KiB, free 433.8 MiB)\n",
      "25/12/28 17:50:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f30b651f709e:41661 (size: 84.6 KiB, free: 434.2 MiB)\n",
      "25/12/28 17:50:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/28 17:50:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/28 17:50:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/12/28 17:50:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (f30b651f709e, executor driver, partition 0, PROCESS_LOCAL, 7485 bytes) \n",
      "25/12/28 17:50:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/12/28 17:50:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/12/28 17:50:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/12/28 17:50:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "25/12/28 17:50:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f30b651f709e:41661 in memory (size: 84.6 KiB, free: 434.3 MiB)\n",
      "25/12/28 17:50:44 INFO JDBCRDD: closed connection\n",
      "25/12/28 17:50:44 INFO FileOutputCommitter: Saved output of task 'attempt_202512281750298762717988861414030_0001_m_000000_1' to file:/opt/spark/work-dir/processed_data_sample/_temporary/0/task_202512281750298762717988861414030_0001_m_000000\n",
      "25/12/28 17:50:44 INFO SparkHadoopMapRedUtil: attempt_202512281750298762717988861414030_0001_m_000000_1: Committed. Elapsed time: 44 ms.\n",
      "25/12/28 17:50:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2583 bytes result sent to driver\n",
      "25/12/28 17:50:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 14548 ms on f30b651f709e (executor driver) (1/1)\n",
      "25/12/28 17:50:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/12/28 17:50:44 INFO DAGScheduler: ResultStage 1 (csv at <unknown>:0) finished in 14.596 s\n",
      "25/12/28 17:50:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/28 17:50:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/12/28 17:50:44 INFO DAGScheduler: Job 1 finished: csv at <unknown>:0, took 14.609477 s\n",
      "25/12/28 17:50:44 INFO FileFormatWriter: Start to commit write Job e7cb0792-c98d-4398-b3d6-9f2b98ef2e95.\n",
      "25/12/28 17:50:44 INFO FileFormatWriter: Write Job e7cb0792-c98d-4398-b3d6-9f2b98ef2e95 committed. Elapsed time: 192 ms.\n",
      "25/12/28 17:50:44 INFO FileFormatWriter: Finished processing stats for write job e7cb0792-c98d-4398-b3d6-9f2b98ef2e95.\n"
     ]
    }
   ],
   "source": [
    "sampled_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"processed_data_sample\", mode=\"overwrite\")\n",
    "\n",
    "import os \n",
    "for fname in os.listdir(\"processed_data_sample\"):\n",
    "    if fname.startswith(\"part-\") and fname.endswith(\".csv\"):\n",
    "        os.rename(os.path.join(\"processed_data_sample\", fname), \"processed_data_sample.csv\")\n",
    "        break\n",
    "import shutil\n",
    "shutil.rmtree(\"processed_data_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4909fe-8856-476d-b17c-4d89fbc4a4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
